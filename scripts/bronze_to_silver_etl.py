{
	"jobConfig": {
		"name": "4covid",
		"description": "",
		"role": "arn:aws:iam::166725663646:role/glue_role",
		"command": "glueetl",
		"version": "5.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 10,
		"maxCapacity": 10,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 480,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "4covid.py",
		"scriptLocation": "s3://aws-glue-assets-166725663646-us-east-2/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2025-09-06T18:54:06.656Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-166725663646-us-east-2/temporary/",
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-166725663646-us-east-2/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null,
		"logging": false
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import sys\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.sql import SparkSession, functions as F\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\n# --- Glue setup ---\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\njob.init(args['JOB_NAME'], args)\n\n# --- Paths ---\nBUCKET = \"covid-analytics-niroj\"   # <-- your bucket name\nBRONZE = f\"s3://{BUCKET}/covid/bronze\"\nSILVER = f\"s3://{BUCKET}/covid/silver\"\n\n# --- Helpers ---\nupper_trim = lambda c: F.upper(F.trim(F.col(c)))\n\n# ==============================\n# States Lookup\n# ==============================\ntry:\n    print(\"Reading states_abv.csv ...\")\n    states = (spark.read.option(\"header\", True).csv(f\"{BRONZE}/states_abv.csv\")\n              .select(\n                  upper_trim(\"abbreviation\").alias(\"state_code\"),\n                  F.initcap(\"state\").alias(\"state_name\"))\n             )\nexcept Exception as e:\n    print(\"⚠️ Could not read states_abv.csv:\", e)\n    states = None\n\n# ==============================\n# Cases (NYTimes us_states.csv)\n# ==============================\ntry:\n    print(\"Reading us_states.csv ...\")\n    cases_raw = spark.read.option(\"header\", True).csv(f\"{BRONZE}/us_states.csv\")\n\n    if states is not None:\n        cases_std = (cases_raw\n            .withColumn(\"full_date\", F.to_date(\"date\"))\n            .withColumn(\"state_name_raw\", F.initcap(F.col(\"state\")))\n            .join(states, states.state_name == F.col(\"state_name_raw\"), \"left\")\n            .withColumn(\"cases_cum\", F.col(\"cases\").cast(\"long\"))\n            .withColumn(\"deaths_cum\", F.col(\"deaths\").cast(\"long\"))\n            .withColumn(\"year\", F.year(\"full_date\"))\n            .withColumn(\"month\", F.month(\"full_date\"))\n            .withColumn(\"day\", F.dayofmonth(\"full_date\"))\n            .select(\"full_date\", \"state_code\", \"state_name\",\n                    \"cases_cum\", \"deaths_cum\", \"year\", \"month\", \"day\")\n            .dropna(subset=[\"full_date\", \"state_code\"])\n        )\n\n        print(\"Writing cases_standardized to Silver ...\")\n        cases_std.write.mode(\"overwrite\") \\\n            .partitionBy(\"state_code\",\"year\",\"month\",\"day\") \\\n            .parquet(f\"{SILVER}/cases_standardized\")\nexcept Exception as e:\n    print(\"⚠️ Skipping us_states.csv processing:\", e)\n\n# ==============================\n# Testing (COVID Tracking states_daily.csv)\n# ==============================\ntry:\n    print(\"Reading states_daily.csv ...\")\n    tests_raw = spark.read.option(\"header\", True).csv(f\"{BRONZE}/states_daily.csv\")\n\n    if states is not None:\n        tests_std = (tests_raw\n            .withColumn(\"full_date\", F.to_date(F.col(\"date\").cast(\"string\"), \"yyyyMMdd\"))\n            .withColumn(\"state_code\", upper_trim(\"state\"))\n            .join(states.select(\"state_code\", \"state_name\"), \"state_code\", \"left\")\n            .withColumn(\"tests_total_cum\", F.col(\"totalTestResults\").cast(\"long\"))\n            .withColumn(\"tests_pos_cum\", F.col(\"positive\").cast(\"long\"))\n            .withColumn(\"tests_neg_cum\", F.col(\"negative\").cast(\"long\"))\n            .withColumn(\"year\", F.year(\"full_date\"))\n            .withColumn(\"month\", F.month(\"full_date\"))\n            .withColumn(\"day\", F.dayofmonth(\"full_date\"))\n            .select(\"full_date\",\"state_code\",\"state_name\",\n                    \"tests_total_cum\",\"tests_pos_cum\",\"tests_neg_cum\",\n                    \"year\",\"month\",\"day\")\n            .dropna(subset=[\"full_date\", \"state_code\"])\n        )\n\n        print(\"Writing testing_standardized to Silver ...\")\n        tests_std.write.mode(\"overwrite\") \\\n            .partitionBy(\"state_code\",\"year\",\"month\",\"day\") \\\n            .parquet(f\"{SILVER}/testing_standardized\")\nexcept Exception as e:\n    print(\"⚠️ Skipping states_daily.csv processing:\", e)\n\nprint(\"✅ Silver layer complete.\")\njob.commit()\n"
}